{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05de4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification\n",
    "\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def clean_stopwords_shortwords(w):\n",
    "    stopwords_list=stopwords.words('english')\n",
    "    words = w.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 2]\n",
    "    return \" \".join(clean_words) \n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w=clean_stopwords_shortwords(w)\n",
    "    w=re.sub(r'@\\w+', '',w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6abdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bb98bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes=len(train.target.unique())\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e447a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'to', 'train', 'the', 'model', ',', 'lets', 'look', 'at', 'how', 'a', 'trained', 'model', 'calculate', '##s', 'its', 'prediction', '.']\n"
     ]
    }
   ],
   "source": [
    "sent= 'how to train the model, lets look at how a trained model calculates its prediction.'\n",
    "tokens=bert_tokenizer.tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fdf7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 7613)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = train['text']\n",
    "labels = train['target']\n",
    "len(sentences),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeac84f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Doubl\\Desktop\\Projects\\nlp\\disaster_tweets\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in sentences:\n",
    "    bert_inp = bert_tokenizer.encode_plus(\n",
    "        sent,\n",
    "        add_special_tokens = True,\n",
    "        max_length = 90,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True\n",
    "    )\n",
    "    \n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids = np.asarray(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f743167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 7613, 7613)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids),len(attention_masks),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0752e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inp shape (6090, 90) Val input shape (1523, 90)\n",
      "Train label shape (6090,) Val label shape (1523,)\n",
      "Train attention mask shape (6090, 90) Val attention mask shape (1523, 90)\n"
     ]
    }
   ],
   "source": [
    "train_inp , val_inp, train_label, val_label, train_mask, val_mask = train_test_split(\n",
    "    input_ids,\n",
    "    labels,\n",
    "    attention_masks,\n",
    "    test_size = 0.2,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(\n",
    "    train_inp.shape,\n",
    "    val_inp.shape,\n",
    "    train_label.shape,\n",
    "    val_label.shape,\n",
    "    train_mask.shape,\n",
    "    val_mask.shape\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07eaffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'tensorboard_data/tb_bert'\n",
    "model_save_path = './models/bert_model.h5'\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_save_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    "),keras.callbacks.TensorBoard(log_dir = log_dir)]\n",
    "\n",
    "print('\\nBert Model',bert_model.summary())\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe077e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "381/381 [==============================] - 64s 145ms/step - loss: 0.4429 - accuracy: 0.8031 - val_loss: 0.3774 - val_accuracy: 0.8411\n",
      "Epoch 2/10\n",
      "381/381 [==============================] - 56s 146ms/step - loss: 0.3180 - accuracy: 0.8724 - val_loss: 0.3851 - val_accuracy: 0.8372\n",
      "Epoch 3/10\n",
      "381/381 [==============================] - 54s 142ms/step - loss: 0.2210 - accuracy: 0.9190 - val_loss: 0.4697 - val_accuracy: 0.8201\n",
      "Epoch 4/10\n",
      "381/381 [==============================] - 56s 147ms/step - loss: 0.1450 - accuracy: 0.9514 - val_loss: 0.5312 - val_accuracy: 0.8273\n",
      "Epoch 5/10\n",
      "381/381 [==============================] - 55s 145ms/step - loss: 0.1114 - accuracy: 0.9611 - val_loss: 0.6171 - val_accuracy: 0.8273\n",
      "Epoch 6/10\n",
      "381/381 [==============================] - 56s 146ms/step - loss: 0.0806 - accuracy: 0.9704 - val_loss: 0.5713 - val_accuracy: 0.8286\n",
      "Epoch 7/10\n",
      "381/381 [==============================] - 55s 145ms/step - loss: 0.0709 - accuracy: 0.9731 - val_loss: 0.7737 - val_accuracy: 0.7873\n",
      "Epoch 8/10\n",
      "381/381 [==============================] - 56s 146ms/step - loss: 0.0626 - accuracy: 0.9742 - val_loss: 0.7142 - val_accuracy: 0.8221\n",
      "Epoch 9/10\n",
      "381/381 [==============================] - 56s 146ms/step - loss: 0.0534 - accuracy: 0.9773 - val_loss: 0.8561 - val_accuracy: 0.8135\n",
      "Epoch 10/10\n",
      "381/381 [==============================] - 55s 145ms/step - loss: 0.0449 - accuracy: 0.9806 - val_loss: 0.9466 - val_accuracy: 0.8109\n"
     ]
    }
   ],
   "source": [
    "history=bert_model.fit(\n",
    "    [train_inp, train_mask],\n",
    "    train_label,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    validation_data=([val_inp,val_mask],val_label),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eeef056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a218ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir='tensorboard_data/bert_model'\n",
    "# %tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1521f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[ 0.93973464, -0.6765221 ],\n",
       "       [ 1.2597959 , -0.986002  ],\n",
       "       [ 1.3038394 , -0.93121046],\n",
       "       ...,\n",
       "       [ 1.2953413 , -0.94678944],\n",
       "       [ 0.6063849 , -0.3072171 ],\n",
       "       [ 1.1619651 , -0.8398766 ]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
    "trained_model.load_weights(model_save_path)\n",
    "\n",
    "preds = trained_model.predict([val_inp,val_mask],batch_size=32)\n",
    "preds\n",
    "# pred_labels = preds.argmax(axis=1)\n",
    "# f1 = f1_score(val_label,pred_labels)\n",
    "# print('F1 score',f1)\n",
    "# print('Classification Report')\n",
    "# print(classification_report(val_label,pred_labels,target_names=target_names))\n",
    "\n",
    "# print('Training and saving built model.....')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c4847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster-tweets",
   "language": "python",
   "name": "disaster-tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
