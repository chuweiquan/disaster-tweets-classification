{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05de4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964addc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1286cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a9e1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f67c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(\"\")\n",
    "train['new_text'] = train['keyword'] + \" \" + train['location'] + \" \" + train['text']\n",
    "\n",
    "test = test.fillna(\"\")\n",
    "test['new_text'] = test['keyword'] + \" \" + test['location'] + \" \" + test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8a9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: (7613, 6)\n",
      "Size of test dataset: (3263, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of train dataset: \" + str(train.shape))\n",
    "print(\"Size of test dataset: \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecfa70",
   "metadata": {},
   "source": [
    "### Use GPU if its available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29c1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available\n",
      "We will use the GPU: NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    print(\"There are %d GPU(s) available\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU Available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56ba40",
   "metadata": {},
   "source": [
    "### Loop to remove noise from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a870f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(df):\n",
    "    df = df.copy()\n",
    "    new_tweets = []\n",
    "    for text in df['new_text']:\n",
    "        # it will remove the old style retweet text \"RT\"\n",
    "        new_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "        # it will remove hyperlinks\n",
    "        new_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', new_text)\n",
    "\n",
    "        # it will remove hashtags. We have to be careful here not to remove \n",
    "        # the whole hashtag because text of hashtags contains huge information. \n",
    "        # only removing the hash # sign from the word\n",
    "        # tweet2 = re.sub(r'#', '', tweet2)\n",
    "        new_text = ''.join([c for c in new_text if ord(c) < 128])\n",
    "\n",
    "        # it will remove single numeric terms in the tweet. \n",
    "#         new_text = re.sub(r'[0-9]', '', new_text) # does temporal information matter to bert? lets test it out\n",
    "#         print('\\nAfter removing old style tweet, hyperlinks and # sign')\n",
    "        new_tweets.append(new_text)\n",
    "    \n",
    "    df['new_text'] = new_tweets\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764386be",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = train.copy()\n",
    "new_train = remove_noise(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9633b48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"apocalypse Elk Grove, CA, USA Another hour! It's August 05 2015 at 08:02PM Here's Red Rover Zombie Apocalypse 2014!  #internetradio #collegeradi_\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train[new_train['id'] == 404]['new_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05a4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"apocalypse Elk Grove, CA, USA Another hour! It's August 05 2015 at 08:02PM Here's Red Rover Zombie Apocalypse 2014! http://t.co/cf9e6TU3g7 #internetradio #collegeradi\\x89Ã›_\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['id'] == 404]['new_text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d226c",
   "metadata": {},
   "source": [
    "### Initialize tokenizer and finding the average length of tokens for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9346b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c4fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Min Length: 4\n",
      "Median Length: 28.0\n",
      "Max Length: 93\n",
      "\n",
      "Test\n",
      "Min Length: 4\n",
      "Median Length: 38.0\n",
      "Max Length: 82\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "test_inputs = []\n",
    "train_length = []\n",
    "test_length = []\n",
    "\n",
    "# for sentence in train['text']:\n",
    "for sentence in new_train['new_text']:\n",
    "    encoded_sentence = tokenizer.encode(sentence, add_special_tokens = True)\n",
    "    train_inputs.append(encoded_sentence)\n",
    "    train_length.append(len(encoded_sentence))\n",
    "\n",
    "for sentence in test['new_text']:\n",
    "    encoded_sentence = tokenizer.encode(sentence, add_special_tokens = True)\n",
    "    test_inputs.append(encoded_sentence)\n",
    "    test_length.append(len(encoded_sentence))\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"Min Length:\", min(train_length))\n",
    "print(\"Median Length:\", np.median(train_length))\n",
    "print(\"Max Length:\", max(train_length))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Test\")\n",
    "print(\"Min Length:\", min(test_length))\n",
    "print(\"Median Length:\", np.median(test_length))\n",
    "print(\"Max Length:\", max(test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abdba4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doubl\\Desktop\\Projects\\nlp\\disaster_tweets\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b0e233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in new_train['new_text']]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "724f3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ac024",
   "metadata": {},
   "source": [
    "### Distribution of disaster to non-disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c63fcb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disaster tweets: 3271\n",
      "Number of non-disaster tweets: 4342\n",
      "Percentage of disaster to non-disaster tweets: 0.43\n"
     ]
    }
   ],
   "source": [
    "labels = new_train['target']\n",
    "print(\"Number of disaster tweets: \" + str(len(train[train['target'] == 1]['target'])))\n",
    "print(\"Number of non-disaster tweets: \" + str(len(train[train['target'] == 0]['target'])))\n",
    "print(\"Percentage of disaster to non-disaster tweets: \" + str(round(len(train[train['target'] == 1]['target']) / len(train['target']), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30e9a3",
   "metadata": {},
   "source": [
    "### Split train dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d9c3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(train['new_text'], \n",
    "                                                                    train['target'], \n",
    "                                                                    random_state=2022, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=train['target'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2022, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc81961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (5329,)\n",
      "Shape of validation data: (1142,)\n",
      "Shape of test data: (1142,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data:\", str(train_text.shape))\n",
    "print(\"Shape of validation data:\", str(val_text.shape))\n",
    "print(\"Shape of test data:\", str(test_text.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c2f4195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doubl\\Desktop\\Projects\\nlp\\disaster_tweets\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = MAX_LEN,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = MAX_LEN,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = MAX_LEN,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d295106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39602fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "058bc9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# bert = BertForSequenceClassification.from_pretrained(\n",
    "#     'bert-base-uncased',\n",
    "#     num_labels = 2,\n",
    "#     output_attentions = False,\n",
    "#     output_hidden_states = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8d52ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3bf5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict = False)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f650437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cef2090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87676867 1.16353712]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(train_labels), \n",
    "                                 y = train_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e155a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# # number of training epochs\n",
    "# epochs = 5\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 30\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0, # default value in run glue.py\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6abcc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd7d4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1281e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2022\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d360ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    t0 = time.time()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    train_accuracy, nb_train_steps = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(\"Batch {:>5,} of {:>5,}. Elapsed: {:}\".format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        preds = model(sent_id, mask)\n",
    "#         preds = model(sent_id, token_type_ids = None, attention_mask = mask, labels = labels)\n",
    "        \n",
    "#         loss = preds[0]\n",
    "    \n",
    "        logits = preds\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        tmp_train_accuracy = flat_accuracy(logits, label_ids)\n",
    "        train_accuracy += tmp_train_accuracy\n",
    "\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step\n",
    "        \n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "        nb_train_steps += 1\n",
    "        \n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    accuracy = train_accuracy / nb_train_steps\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Training Accuracy: {0:.5f}\".format(accuracy))\n",
    "    print(\"Average training loss: {0:.5f}\".format(avg_loss))\n",
    "    print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, accuracy, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f9cbe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_accuracy, nb_eval_steps = 0, 0\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "\n",
    "#         # Progress update every 50 batches.\n",
    "#         if step % 50 == 0 and not step == 0:\n",
    "\n",
    "#             # Calculate elapsed time in minutes.\n",
    "# #             elapsed = format_time(time.time() - t0)\n",
    "\n",
    "#             # Report progress.\n",
    "#             print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            \n",
    "            logits = preds\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(\"Validation Accuracy: {0:.5f}\".format(accuracy))\n",
    "    print(\"Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, accuracy, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20243d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:03\n",
      "Batch   200 of   334. Elapsed: 0:00:05\n",
      "Batch   300 of   334. Elapsed: 0:00:06\n",
      "\n",
      "Training Accuracy: 0.64577\n",
      "Average training loss: 0.64071\n",
      "Training epoch took: 0:00:07\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.74219\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 2 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.69124\n",
      "Average training loss: 0.58970\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.74913\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 3 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.71725\n",
      "Average training loss: 0.57679\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.75000\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 4 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.72680\n",
      "Average training loss: 0.55363\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.76823\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 5 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.72885\n",
      "Average training loss: 0.54477\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.70949\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 6 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.73054\n",
      "Average training loss: 0.54439\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.77141\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 7 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.74046\n",
      "Average training loss: 0.53421\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.72164\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 8 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.74981\n",
      "Average training loss: 0.52091\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.76562\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 9 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.74944\n",
      "Average training loss: 0.51258\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.78096\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 10 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.75281\n",
      "Average training loss: 0.52426\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.70428\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 11 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.74888\n",
      "Average training loss: 0.52094\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.71644\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 12 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76310\n",
      "Average training loss: 0.51101\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.76968\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 13 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76104\n",
      "Average training loss: 0.50619\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.76447\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 14 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76329\n",
      "Average training loss: 0.51103\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.76128\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 15 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.75468\n",
      "Average training loss: 0.52237\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.77749\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 16 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.75561\n",
      "Average training loss: 0.51598\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.68345\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 17 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.75299\n",
      "Average training loss: 0.51646\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.76071\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 18 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76029\n",
      "Average training loss: 0.51103\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.74855\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 19 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76796\n",
      "Average training loss: 0.50677\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.77575\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 20 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76796\n",
      "Average training loss: 0.50081\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.77228\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 21 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76834\n",
      "Average training loss: 0.49895\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.68779\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 22 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76665\n",
      "Average training loss: 0.50114\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.73900\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 23 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.75599\n",
      "Average training loss: 0.50268\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.69068\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 24 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.75674\n",
      "Average training loss: 0.51099\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.67998\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 25 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.77021\n",
      "Average training loss: 0.50060\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.78183\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 26 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76272\n",
      "Average training loss: 0.49661\n",
      "Training epoch took: 0:00:05\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.71470\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 27 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76591\n",
      "Average training loss: 0.50589\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.78443\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 28 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.77040\n",
      "Average training loss: 0.49820\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.78617\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 29 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.76179\n",
      "Average training loss: 0.50103\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.76534\n",
      "Validation took: 0:00:01\n",
      "\n",
      " Epoch 30 / 30\n",
      "Batch   100 of   334. Elapsed: 0:00:02\n",
      "Batch   200 of   334. Elapsed: 0:00:03\n",
      "Batch   300 of   334. Elapsed: 0:00:05\n",
      "\n",
      "Training Accuracy: 0.77096\n",
      "Average training loss: 0.49607\n",
      "Training epoch took: 0:00:06\n",
      "\n",
      "Evaluating...\n",
      "Validation Accuracy: 0.77778\n",
      "Validation took: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, train_accuracy, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, val_accuracy, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_acc.append(train_accuracy)\n",
    "    val_acc.append(val_accuracy)\n",
    "    \n",
    "#     print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "#     print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b04b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dict = {\n",
    "    \"train_accuracy\": train_acc,\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_accuracy\": val_acc,\n",
    "    \"val_loss\": valid_losses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3297706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.645771</td>\n",
       "      <td>0.640706</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.547003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.691243</td>\n",
       "      <td>0.589701</td>\n",
       "      <td>0.749132</td>\n",
       "      <td>0.520549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717253</td>\n",
       "      <td>0.576791</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.516553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.726796</td>\n",
       "      <td>0.553628</td>\n",
       "      <td>0.768229</td>\n",
       "      <td>0.522814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.728855</td>\n",
       "      <td>0.544772</td>\n",
       "      <td>0.709491</td>\n",
       "      <td>0.530353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.730539</td>\n",
       "      <td>0.544395</td>\n",
       "      <td>0.771412</td>\n",
       "      <td>0.545599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.740457</td>\n",
       "      <td>0.534210</td>\n",
       "      <td>0.721644</td>\n",
       "      <td>0.515597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.749813</td>\n",
       "      <td>0.520910</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.495558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.749439</td>\n",
       "      <td>0.512579</td>\n",
       "      <td>0.780961</td>\n",
       "      <td>0.509232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.752807</td>\n",
       "      <td>0.524265</td>\n",
       "      <td>0.704282</td>\n",
       "      <td>0.529129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.748877</td>\n",
       "      <td>0.520936</td>\n",
       "      <td>0.716435</td>\n",
       "      <td>0.536292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.763099</td>\n",
       "      <td>0.511009</td>\n",
       "      <td>0.769676</td>\n",
       "      <td>0.491487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.761040</td>\n",
       "      <td>0.506194</td>\n",
       "      <td>0.764468</td>\n",
       "      <td>0.496871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.763286</td>\n",
       "      <td>0.511035</td>\n",
       "      <td>0.761285</td>\n",
       "      <td>0.496457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.754678</td>\n",
       "      <td>0.522371</td>\n",
       "      <td>0.777488</td>\n",
       "      <td>0.490209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.755614</td>\n",
       "      <td>0.515975</td>\n",
       "      <td>0.683449</td>\n",
       "      <td>0.597741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.752994</td>\n",
       "      <td>0.516461</td>\n",
       "      <td>0.760706</td>\n",
       "      <td>0.497815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.760292</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>0.748553</td>\n",
       "      <td>0.529216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.767964</td>\n",
       "      <td>0.506772</td>\n",
       "      <td>0.775752</td>\n",
       "      <td>0.488917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.767964</td>\n",
       "      <td>0.500809</td>\n",
       "      <td>0.772280</td>\n",
       "      <td>0.487001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.768338</td>\n",
       "      <td>0.498948</td>\n",
       "      <td>0.687789</td>\n",
       "      <td>0.547338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.766654</td>\n",
       "      <td>0.501145</td>\n",
       "      <td>0.739005</td>\n",
       "      <td>0.512272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.755988</td>\n",
       "      <td>0.502678</td>\n",
       "      <td>0.690683</td>\n",
       "      <td>0.543229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.756737</td>\n",
       "      <td>0.510991</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.616316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.770210</td>\n",
       "      <td>0.500605</td>\n",
       "      <td>0.781829</td>\n",
       "      <td>0.478776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.762725</td>\n",
       "      <td>0.496612</td>\n",
       "      <td>0.714699</td>\n",
       "      <td>0.523369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.765906</td>\n",
       "      <td>0.505887</td>\n",
       "      <td>0.784433</td>\n",
       "      <td>0.498674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.770397</td>\n",
       "      <td>0.498199</td>\n",
       "      <td>0.786169</td>\n",
       "      <td>0.497302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.761789</td>\n",
       "      <td>0.501028</td>\n",
       "      <td>0.765336</td>\n",
       "      <td>0.501316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.770958</td>\n",
       "      <td>0.496070</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.479646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_accuracy  train_loss  val_accuracy  val_loss\n",
       "0         0.645771    0.640706      0.742188  0.547003\n",
       "1         0.691243    0.589701      0.749132  0.520549\n",
       "2         0.717253    0.576791      0.750000  0.516553\n",
       "3         0.726796    0.553628      0.768229  0.522814\n",
       "4         0.728855    0.544772      0.709491  0.530353\n",
       "5         0.730539    0.544395      0.771412  0.545599\n",
       "6         0.740457    0.534210      0.721644  0.515597\n",
       "7         0.749813    0.520910      0.765625  0.495558\n",
       "8         0.749439    0.512579      0.780961  0.509232\n",
       "9         0.752807    0.524265      0.704282  0.529129\n",
       "10        0.748877    0.520936      0.716435  0.536292\n",
       "11        0.763099    0.511009      0.769676  0.491487\n",
       "12        0.761040    0.506194      0.764468  0.496871\n",
       "13        0.763286    0.511035      0.761285  0.496457\n",
       "14        0.754678    0.522371      0.777488  0.490209\n",
       "15        0.755614    0.515975      0.683449  0.597741\n",
       "16        0.752994    0.516461      0.760706  0.497815\n",
       "17        0.760292    0.511027      0.748553  0.529216\n",
       "18        0.767964    0.506772      0.775752  0.488917\n",
       "19        0.767964    0.500809      0.772280  0.487001\n",
       "20        0.768338    0.498948      0.687789  0.547338\n",
       "21        0.766654    0.501145      0.739005  0.512272\n",
       "22        0.755988    0.502678      0.690683  0.543229\n",
       "23        0.756737    0.510991      0.679977  0.616316\n",
       "24        0.770210    0.500605      0.781829  0.478776\n",
       "25        0.762725    0.496612      0.714699  0.523369\n",
       "26        0.765906    0.505887      0.784433  0.498674\n",
       "27        0.770397    0.498199      0.786169  0.497302\n",
       "28        0.761789    0.501028      0.765336  0.501316\n",
       "29        0.770958    0.496070      0.777778  0.479646"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(r_dict)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4a077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster-tweets",
   "language": "python",
   "name": "disaster-tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
