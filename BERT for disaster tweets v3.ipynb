{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05de4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964addc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1286cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a9e1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f67c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(\"\")\n",
    "train['new_text'] = train['keyword'] + \" \" + train['location'] + \" \" + train['text']\n",
    "\n",
    "test = test.fillna(\"\")\n",
    "test['new_text'] = test['keyword'] + \" \" + test['location'] + \" \" + test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8a9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: (7613, 6)\n",
      "Size of test dataset: (3263, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of train dataset: \" + str(train.shape))\n",
    "print(\"Size of test dataset: \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecfa70",
   "metadata": {},
   "source": [
    "### Use GPU if its available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29c1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available\n",
      "We will use the GPU: NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    print(\"There are %d GPU(s) available\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU Available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56ba40",
   "metadata": {},
   "source": [
    "### Loop to remove noise from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a870f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(df):\n",
    "    df = df.copy()\n",
    "    new_tweets = []\n",
    "    for text in df['new_text']:\n",
    "        # it will remove the old style retweet text \"RT\"\n",
    "        new_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "        # it will remove hyperlinks\n",
    "        new_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', new_text)\n",
    "\n",
    "        # it will remove hashtags. We have to be careful here not to remove \n",
    "        # the whole hashtag because text of hashtags contains huge information. \n",
    "        # only removing the hash # sign from the word\n",
    "        # tweet2 = re.sub(r'#', '', tweet2)\n",
    "        new_text = ''.join([c for c in new_text if ord(c) < 128])\n",
    "\n",
    "        # it will remove single numeric terms in the tweet. \n",
    "#         new_text = re.sub(r'[0-9]', '', new_text) # does temporal information matter to bert? lets test it out\n",
    "#         print('\\nAfter removing old style tweet, hyperlinks and # sign')\n",
    "        new_tweets.append(new_text)\n",
    "    \n",
    "    df['new_text'] = new_tweets\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764386be",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = train.copy()\n",
    "new_train = remove_noise(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9633b48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"apocalypse Elk Grove, CA, USA Another hour! It's August 05 2015 at 08:02PM Here's Red Rover Zombie Apocalypse 2014!  #internetradio #collegeradi_\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train[new_train['id'] == 404]['new_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05a4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"apocalypse Elk Grove, CA, USA Another hour! It's August 05 2015 at 08:02PM Here's Red Rover Zombie Apocalypse 2014! http://t.co/cf9e6TU3g7 #internetradio #collegeradi\\x89Û_\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['id'] == 404]['new_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee67550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet = train[train['id'] == 219]['text'].values[0]\n",
    "# tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "878a80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # it will remove the old style retweet text \"RT\"\n",
    "# tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# # it will remove hyperlinks\n",
    "# tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# # it will remove hashtags. We have to be careful here not to remove \n",
    "# # the whole hashtag because text of hashtags contains huge information. \n",
    "# # only removing the hash # sign from the word\n",
    "# # tweet2 = re.sub(r'#', '', tweet2)\n",
    "# tweet2 = ''.join([c for c in tweet2 if ord(c) < 128])\n",
    "\n",
    "# # it will remove single numeric terms in the tweet. \n",
    "# tweet2 = re.sub(r'[0-9]', '', tweet2) # does temporal information matter to bert? lets test it out\n",
    "# print('\\nAfter removing old style tweet, hyperlinks and # sign')\n",
    "# tweet2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8ebcf",
   "metadata": {},
   "source": [
    "### Split train dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28486574",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(train['new_text'], \n",
    "                                                                    train['target'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=train['target'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c913109",
   "metadata": {},
   "source": [
    "### Initialize tokenizer and finding the average length of tokens for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84807b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8d93d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAATqElEQVR4nO3df4xdZZ3H8fdXCkoYtgOBnTRtd8sujYbQFeksYjRmBqIpsNmyCbIYVgph090NGsxiQvUfdaPZuhtkMTG4XXEpRh0JytIAuktKJy7JglJByg9dKpa1E2yDluqIP7b63T/u02WY3pl7O3Pnzr1P369kcs95nuee+70n0889fc65ZyIzkSTV5TWLXYAkqfMMd0mqkOEuSRUy3CWpQoa7JFVoyWIXAHDaaaflqlWrjmj/+c9/zkknndT9gubBmruj32rut3rBmrtlPjXv3Lnzxcw8vWlnZi76z9q1a7OZHTt2NG3vZdbcHf1Wc7/Vm2nN3TKfmoFHc4ZcdVpGkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIq1BO3H5B61apN97U9ds/mSxawEunoeOQuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVqK1wj4jBiLgrIr4bEc9ExFsi4tSIeCAini2Pp5SxERGfiojdEfFERJy7sG9BkjRdu0futwBfz8w3AG8EngE2AdszczWwvawDXASsLj8bgVs7WrEkqaWW4R4RS4G3A7cBZOavM/MlYD2wtQzbClxaltcDd2TDw8BgRCzrcN2SpFlEZs4+IOIcYAvwNI2j9p3A9cBEZg6WMQEcyMzBiLgX2JyZD5W+7cCNmfnotO1upHFkz9DQ0NqxsbEjXntycpKBgYH5vL+us+bu6FbNuyYOtj12zfKlM/a5j7vjWKt5dHR0Z2YON+tr537uS4Bzgfdl5iMRcQuvTMEAkJkZEbN/SkyTmVtofGgwPDycIyMjR4wZHx+nWXsvs+bu6FbNVx/N/dyvHJmxz33cHdb8inbm3PcCezPzkbJ+F42w33d4uqU87i/9E8DKKc9fUdokSV3SMtwz80fADyPi9aXpQhpTNNuADaVtA3BPWd4GXFWumjkfOJiZL3S2bEnSbNr9M3vvA74QEScAzwHX0PhguDMirgWeBy4vY+8HLgZ2Ay+XsZKkLmor3DPzcaDZpP2FTcYmcN38ypIkzYffUJWkChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQm2Fe0TsiYhdEfF4RDxa2k6NiAci4tnyeEppj4j4VETsjognIuLchXwDkqQjHc2R+2hmnpOZw2V9E7A9M1cD28s6wEXA6vKzEbi1U8VKktozn2mZ9cDWsrwVuHRK+x3Z8DAwGBHL5vE6kqSjFJnZelDED4ADQAL/nJlbIuKlzBws/QEcyMzBiLgX2JyZD5W+7cCNmfnotG1upHFkz9DQ0NqxsbEjXndycpKBgYH5vL+us+bu6FbNuyYOtj12zfKlM/a5j7vjWKt5dHR055TZlFdZ0uY23paZExHxu8ADEfHdqZ2ZmRHR+lPi1c/ZAmwBGB4ezpGRkSPGjI+P06y9l1lzd3Sr5qs33df22D1XjszY5z7uDmt+RVvTMpk5UR73A3cD5wH7Dk+3lMf9ZfgEsHLK01eUNklSl7QM94g4KSJOPrwMvBN4EtgGbCjDNgD3lOVtwFXlqpnzgYOZ+ULHK5ckzaidaZkh4O7GtDpLgC9m5tcj4lvAnRFxLfA8cHkZfz9wMbAbeBm4puNVS5Jm1TLcM/M54I1N2n8MXNikPYHrOlKdJGlO/IaqJFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRVqO9wj4riIeCwi7i3rZ0TEIxGxOyK+HBEnlPbXlvXdpX/VAtUuSZrB0Ry5Xw88M2X9E8DNmXkmcAC4trRfCxwo7TeXcZKkLmor3CNiBXAJ8NmyHsAFwF1lyFbg0rK8vqxT+i8s4yVJXRKZ2XpQxF3A3wMnAx8ArgYeLkfnRMRK4GuZeXZEPAmsy8y9pe/7wJsz88Vp29wIbAQYGhpaOzY2dsTrTk5OMjAwMPd3twisuTu6VfOuiYNtj12zfOmMfe7j7jjWah4dHd2ZmcPN+pa0enJE/AmwPzN3RsTInCpoIjO3AFsAhoeHc2TkyE2Pj4/TrL2XWXN3dKvmqzfd1/bYPVeOzNjnPu4Oa35Fy3AH3gr8aURcDLwO+B3gFmAwIpZk5iFgBTBRxk8AK4G9EbEEWAr8uOOVS5Jm1HLOPTM/mJkrMnMVcAXwYGZeCewALivDNgD3lOVtZZ3S/2C2M/cjSeqYdo7cZ3IjMBYRHwMeA24r7bcBn4+I3cBPaHwgaBGsanNK4fZ1Jy1wJZK67ajCPTPHgfGy/BxwXpMxvwTe1YHaJElz5DdUJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUoaP6A9lSp63adF9b4/ZsvmSBK5HqYrhLXeYHmrrBaRlJqlDLcI+I10XENyPiOxHxVER8tLSfERGPRMTuiPhyRJxQ2l9b1neX/lUL/B4kSdO0My3zK+CCzJyMiOOBhyLia8DfAjdn5lhEfAa4Fri1PB7IzDMj4grgE8CfL1D9Us+YbbrlhjWHuLrN6RipE1oeuWfDZFk9vvwkcAFwV2nfClxalteXdUr/hRERnSpYktRaZGbrQRHHATuBM4FPA/8IPJyZZ5b+lcDXMvPsiHgSWJeZe0vf94E3Z+aL07a5EdgIMDQ0tHZsbOyI152cnGRgYGAeb6/7eqnmXRMH2xp3xtLjFq3mdmtcs3zpq9a7tZ/bra+VoRNh3y+O7jnT33O39dLvcruOtZpHR0d3ZuZws762rpbJzN8A50TEIHA38IY5VfLqbW4BtgAMDw/nyMjIEWPGx8dp1t7LeqnmdqcBbl930qLV3G6Ne64cedV6t/Zzp6ZSblhziJt2Hd3FadPfc7f10u9yu6z5FUf125aZL0XEDuAtwGBELMnMQ8AKYKIMmwBWAnsjYgmwFPhxB2tWH2j3cr+5bm+mOWwvH5QaWoZ7RJwO/G8J9hOBd9A4SboDuAwYAzYA95SnbCvr/1X6H8x25n6kLur0h4/Ua9o5cl8GbC3z7q8B7szMeyPiaWAsIj4GPAbcVsbfBnw+InYDPwGuWIC6JUmzaBnumfkE8KYm7c8B5zVp/yXwro5UJ0maE7+hKkkV8t4yqopz6VKDR+6SVCHDXZIqZLhLUoUMd0mqkCdUJc3J0Zy89pvD3eeRuyRVyHCXpAoZ7pJUIefcdVT8klD3+Ie0NR+Gu9Tn/BBQM07LSFKFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIa9zF7smDnK1X06SquKRuyRVyHCXpAo5LSMdI7xNwbGl5ZF7RKyMiB0R8XREPBUR15f2UyPigYh4tjyeUtojIj4VEbsj4omIOHeh34Qk6dXamZY5BNyQmWcB5wPXRcRZwCZge2auBraXdYCLgNXlZyNwa8erliTNqmW4Z+YLmfntsvwz4BlgObAe2FqGbQUuLcvrgTuy4WFgMCKWdbpwSdLMIjPbHxyxCvgGcDbwP5k5WNoDOJCZgxFxL7A5Mx8qfduBGzPz0Wnb2kjjyJ6hoaG1Y2NjR7ze5OQkAwMDc3hbi6eXat41cbCtcUMnwr5fLHAxHdZvNfdTvWuWLwVa/y63+/s1dZsLrZf+/bVrPjWPjo7uzMzhZn1tn1CNiAHgK8D7M/OnjTxvyMyMiPY/JRrP2QJsARgeHs6RkZEjxoyPj9OsvZf1Us3tXrt+w5pD3LSrv86t91vN/VTvnitHgNa/y0fz3YjD21xovfTvr10LVXNbl0JGxPE0gv0LmfnV0rzv8HRLedxf2ieAlVOevqK0SZK6pJ2rZQK4DXgmMz85pWsbsKEsbwDumdJ+Vblq5nzgYGa+0MGaJUkttPP/xLcC7wF2RcTjpe1DwGbgzoi4FngeuLz03Q9cDOwGXgau6WTBkqTWWoZ7OTEaM3Rf2GR8AtfNsy5J0jz0xxkeSV1z+JusN6w55A3l+pj3lpGkChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCH/zF4fWuWfPpPUgkfuklQhw12SKmS4S1KFWoZ7RHwuIvZHxJNT2k6NiAci4tnyeEppj4j4VETsjognIuLchSxektRcO0futwPrprVtArZn5mpge1kHuAhYXX42Ard2pkxJ0tFoebVMZn4jIlZNa14PjJTlrcA4cGNpvyMzE3g4IgYjYllmvtCxiivmVTCSOiUaOdxiUCPc783Ms8v6S5k5WJYDOJCZgxFxL7A5Mx8qfduBGzPz0Sbb3Ejj6J6hoaG1Y2NjR7zu5OQkAwMDc3xri2M+Ne+aONjhatozdCLs+8WivPSc9VvN/VYvdLbmNcuXdmZDLRxrmTE6OrozM4eb9c37OvfMzIho/Qlx5PO2AFsAhoeHc2Rk5Igx4+PjNGvvZfOp+epFOnK/Yc0hbtrVX1956Lea+61e6GzNe64c6ch2WjnWMmM2c71aZl9ELAMoj/tL+wSwcsq4FaVNktRFcw33bcCGsrwBuGdK+1XlqpnzgYPOt0tS97X8P1dEfInGydPTImIv8GFgM3BnRFwLPA9cXobfD1wM7AZeBq5ZgJolSS20c7XMu2fourDJ2ASum29RkqT58RuqklQhw12SKmS4S1KFDHdJqlB/fatCUl9q99YaezZfssCVHDs8cpekChnuklQhp2W6wLs9Suo2w11Sz3BuvnMM93lo9ot4w5pDi3Z3R0k6zDl3SaqQ4S5JFTLcJalCzrlL6jsznXidfs7rWD7x6pG7JFXIcJekChnuklQhw12SKuQJ1Sa8XYCkfme4S6pWpw/U+unqG6dlJKlChrskVchwl6QKLcice0SsA24BjgM+m5mbF+J1joYnSSUdSzoe7hFxHPBp4B3AXuBbEbEtM5/u9GuBoS1JzSzEkft5wO7MfA4gIsaA9cCChLskdctCHEzevu6kjm8TIDKzsxuMuAxYl5l/WdbfA7w5M987bdxGYGNZfT3wvSabOw14saMFLjxr7o5+q7nf6gVr7pb51Pz7mXl6s45Fu849M7cAW2YbExGPZuZwl0rqCGvujn6rud/qBWvuloWqeSGulpkAVk5ZX1HaJEldshDh/i1gdUScEREnAFcA2xbgdSRJM+j4tExmHoqI9wL/TuNSyM9l5lNz3Nys0zY9ypq7o99q7rd6wZq7ZUFq7vgJVUnS4vMbqpJUIcNdkirUs+EeEesi4nsRsTsiNi12Pe2IiD0RsSsiHo+IRxe7nmYi4nMRsT8inpzSdmpEPBARz5bHUxazxqlmqPcjETFR9vPjEXHxYtY4XUSsjIgdEfF0RDwVEdeX9l7ezzPV3JP7OiJeFxHfjIjvlHo/WtrPiIhHSm58uVzU0RNmqfn2iPjBlH18TkdeMDN77ofGidjvA38AnAB8Bzhrsetqo+49wGmLXUeLGt8OnAs8OaXtH4BNZXkT8InFrrNFvR8BPrDYtc1S8zLg3LJ8MvDfwFk9vp9nqrkn9zUQwEBZPh54BDgfuBO4orR/Bvibxa61jZpvBy7r9Ov16pH7/9/CIDN/DRy+hYHmKTO/AfxkWvN6YGtZ3gpc2s2aZjNDvT0tM1/IzG+X5Z8BzwDL6e39PFPNPSkbJsvq8eUngQuAu0p7r+3jmWpeEL0a7suBH05Z30sP/6JNkcB/RMTOcnuFfjGUmS+U5R8BQ4tZTJveGxFPlGmbnpnemC4iVgFvonGU1hf7eVrN0KP7OiKOi4jHgf3AAzT+t/9SZh4qQ3ouN6bXnJmH9/HHyz6+OSJe24nX6tVw71dvy8xzgYuA6yLi7Ytd0NHKxv8Ze/362FuBPwTOAV4AblrUamYQEQPAV4D3Z+ZPp/b16n5uUnPP7uvM/E1mnkPjW/DnAW9Y3Ipam15zRJwNfJBG7X8MnArc2InX6tVw78tbGGTmRHncD9xN4xeuH+yLiGUA5XH/Itczq8zcV/6R/Bb4F3pwP0fE8TRC8guZ+dXS3NP7uVnN/bCvM/MlYAfwFmAwIg5/ObNnc2NKzevKlFhm5q+Af6VD+7hXw73vbmEQESdFxMmHl4F3Ak/O/qyesQ3YUJY3APcsYi0tHQ7I4s/osf0cEQHcBjyTmZ+c0tWz+3mmmnt1X0fE6RExWJZPpPH3I56hEZiXlWG9to+b1fzdKR/4QeMcQUf2cc9+Q7VccvVPvHILg48vbkWzi4g/oHG0Do3bOnyxF2uOiC8BIzRuM7oP+DDwbzSuMvg94Hng8szsiZOYM9Q7QmOaIGlcofRXU+ayF11EvA34T2AX8NvS/CEac9i9up9nqvnd9OC+jog/onHC9DgaB6l3ZubflX+HYzSmNx4D/qIcES+6WWp+EDidxtU0jwN/PeXE69xfr1fDXZI0d706LSNJmgfDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXo/wA5TftaFEniMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4edb2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5318082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doubl\\Desktop\\Projects\\nlp\\disaster_tweets\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "971db727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "# type(tokens_train['attention_mask'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50c59727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ad35bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79f338d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict = False)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5d3588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7d80036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4bd01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87676867 1.16353712]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(train_labels), \n",
    "                                 y = train_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd6e5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "477ec25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9118c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c22a6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    train_accuracy, nb_train_steps = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "#         print(type(sent_id))\n",
    "#         print(type(mask))\n",
    "#         print(type(labels))\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        \n",
    "        logits = preds\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        tmp_train_accuracy = flat_accuracy(logits, label_ids)\n",
    "        train_accuracy += tmp_train_accuracy\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "        nb_train_steps += 1\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    accuracy = train_accuracy / nb_train_steps\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Training Accuracy: {0:.5f}\".format(accuracy))\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, accuracy, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e8ddb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    eval_accuracy, nb_eval_steps = 0, 0\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Calculate elapsed time in minutes.\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            \n",
    "            logits = preds\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    print(\"\")\n",
    "    accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(\"Validation Accuracy: {0:.5f}\".format(accuracy))\n",
    "    \n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, accuracy, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "976d6be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.64671\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.73206\n",
      "Training Loss: 0.636\n",
      "Validation Loss: 0.558\n",
      "\n",
      " Epoch 2 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.68189\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.73727\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.542\n",
      "\n",
      " Epoch 3 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.69461\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.75984\n",
      "Training Loss: 0.588\n",
      "Validation Loss: 0.543\n",
      "\n",
      " Epoch 4 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.71445\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.75029\n",
      "Training Loss: 0.581\n",
      "Validation Loss: 0.520\n",
      "\n",
      " Epoch 5 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.72006\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.67766\n",
      "Training Loss: 0.561\n",
      "Validation Loss: 0.565\n",
      "\n",
      " Epoch 6 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.72923\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.73409\n",
      "Training Loss: 0.553\n",
      "Validation Loss: 0.523\n",
      "\n",
      " Epoch 7 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.73129\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.76881\n",
      "Training Loss: 0.547\n",
      "Validation Loss: 0.516\n",
      "\n",
      " Epoch 8 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.72586\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.75405\n",
      "Training Loss: 0.556\n",
      "Validation Loss: 0.557\n",
      "\n",
      " Epoch 9 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.73821\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.72541\n",
      "Training Loss: 0.542\n",
      "Validation Loss: 0.537\n",
      "\n",
      " Epoch 10 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.73802\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.69850\n",
      "Training Loss: 0.539\n",
      "Validation Loss: 0.562\n",
      "\n",
      " Epoch 11 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.75299\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.71846\n",
      "Training Loss: 0.529\n",
      "Validation Loss: 0.533\n",
      "\n",
      " Epoch 12 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.74345\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.59780\n",
      "Training Loss: 0.534\n",
      "Validation Loss: 0.670\n",
      "\n",
      " Epoch 13 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.74551\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.76591\n",
      "Training Loss: 0.535\n",
      "Validation Loss: 0.536\n",
      "\n",
      " Epoch 14 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.74757\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.77980\n",
      "Training Loss: 0.524\n",
      "Validation Loss: 0.509\n",
      "\n",
      " Epoch 15 / 15\n",
      "  Batch    50  of    334.\n",
      "  Batch   100  of    334.\n",
      "  Batch   150  of    334.\n",
      "  Batch   200  of    334.\n",
      "  Batch   250  of    334.\n",
      "  Batch   300  of    334.\n",
      "\n",
      "Training Accuracy: 0.76010\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     72.\n",
      "\n",
      "Validation Accuracy: 0.77199\n",
      "Training Loss: 0.518\n",
      "Validation Loss: 0.496\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, train_accuracy, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, val_accuracy, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_acc.append(train_accuracy)\n",
    "    val_acc.append(val_accuracy)\n",
    "    \n",
    "    print(f'Training Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e0e396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dict = {\n",
    "    \"train_accuracy\": train_acc,\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_accuracy\": val_acc,\n",
    "    \"val_loss\": valid_losses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e4ccb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.646707</td>\n",
       "      <td>0.635526</td>\n",
       "      <td>0.732060</td>\n",
       "      <td>0.558236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.681886</td>\n",
       "      <td>0.602934</td>\n",
       "      <td>0.737269</td>\n",
       "      <td>0.542150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.694611</td>\n",
       "      <td>0.587704</td>\n",
       "      <td>0.759838</td>\n",
       "      <td>0.542607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.714446</td>\n",
       "      <td>0.580848</td>\n",
       "      <td>0.750289</td>\n",
       "      <td>0.520255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.720060</td>\n",
       "      <td>0.560827</td>\n",
       "      <td>0.677662</td>\n",
       "      <td>0.565010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.729229</td>\n",
       "      <td>0.553256</td>\n",
       "      <td>0.734086</td>\n",
       "      <td>0.523347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.731287</td>\n",
       "      <td>0.547326</td>\n",
       "      <td>0.768808</td>\n",
       "      <td>0.516024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.725861</td>\n",
       "      <td>0.555922</td>\n",
       "      <td>0.754051</td>\n",
       "      <td>0.556682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.738211</td>\n",
       "      <td>0.541520</td>\n",
       "      <td>0.725405</td>\n",
       "      <td>0.536845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.738024</td>\n",
       "      <td>0.538642</td>\n",
       "      <td>0.698495</td>\n",
       "      <td>0.561817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.752994</td>\n",
       "      <td>0.528924</td>\n",
       "      <td>0.718461</td>\n",
       "      <td>0.532790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.743451</td>\n",
       "      <td>0.534060</td>\n",
       "      <td>0.597801</td>\n",
       "      <td>0.669754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.745509</td>\n",
       "      <td>0.534647</td>\n",
       "      <td>0.765914</td>\n",
       "      <td>0.535875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.747567</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.779803</td>\n",
       "      <td>0.509294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.760105</td>\n",
       "      <td>0.517868</td>\n",
       "      <td>0.771991</td>\n",
       "      <td>0.496232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_accuracy  train_loss  val_accuracy  val_loss\n",
       "0         0.646707    0.635526      0.732060  0.558236\n",
       "1         0.681886    0.602934      0.737269  0.542150\n",
       "2         0.694611    0.587704      0.759838  0.542607\n",
       "3         0.714446    0.580848      0.750289  0.520255\n",
       "4         0.720060    0.560827      0.677662  0.565010\n",
       "5         0.729229    0.553256      0.734086  0.523347\n",
       "6         0.731287    0.547326      0.768808  0.516024\n",
       "7         0.725861    0.555922      0.754051  0.556682\n",
       "8         0.738211    0.541520      0.725405  0.536845\n",
       "9         0.738024    0.538642      0.698495  0.561817\n",
       "10        0.752994    0.528924      0.718461  0.532790\n",
       "11        0.743451    0.534060      0.597801  0.669754\n",
       "12        0.745509    0.534647      0.765914  0.535875\n",
       "13        0.747567    0.523622      0.779803  0.509294\n",
       "14        0.760105    0.517868      0.771991  0.496232"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(r_dict)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ca007fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4efde2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       652\n",
      "           1       0.76      0.73      0.74       490\n",
      "\n",
      "    accuracy                           0.78      1142\n",
      "   macro avg       0.78      0.78      0.78      1142\n",
      "weighted avg       0.78      0.78      0.78      1142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2379d010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>537</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1\n",
       "row_0          \n",
       "0      537  115\n",
       "1      133  357"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31910c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97b471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218e97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ed70a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "addf2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs = []\n",
    "# test_inputs = []\n",
    "# train_length = []\n",
    "# test_length = []\n",
    "\n",
    "# # for sentence in train['text']:\n",
    "# for sentence in new_train['new_text']:\n",
    "#     encoded_sentence = tokenizer.encode(sentence, add_special_tokens = True)\n",
    "#     train_inputs.append(encoded_sentence)\n",
    "#     train_length.append(len(encoded_sentence))\n",
    "\n",
    "# for sentence in test['new_text']:\n",
    "#     encoded_sentence = tokenizer.encode(sentence, add_special_tokens = True)\n",
    "#     test_inputs.append(encoded_sentence)\n",
    "#     test_length.append(len(encoded_sentence))\n",
    "\n",
    "# print(\"Train\")\n",
    "# print(\"Min Length:\", min(train_length))\n",
    "# print(\"Median Length:\", np.median(train_length))\n",
    "# print(\"Max Length:\", max(train_length))\n",
    "# print(\"\")\n",
    "\n",
    "# print(\"Test\")\n",
    "# print(\"Min Length:\", min(test_length))\n",
    "# print(\"Median Length:\", np.median(test_length))\n",
    "# print(\"Max Length:\", max(test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b558019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(train_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6db89c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b12b5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f7ed7",
   "metadata": {},
   "source": [
    "### Distribution of disaster to non-disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ab18195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = new_train['target']\n",
    "# print(\"Number of disaster tweets: \" + str(len(train[train['target'] == 1]['target'])))\n",
    "# print(\"Number of non-disaster tweets: \" + str(len(train[train['target'] == 0]['target'])))\n",
    "# print(\"Percentage of disaster to non-disaster tweets: \" + str(round(len(train[train['target'] == 1]['target']) / len(train['target']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5d0c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = new_train.drop(columns = ['target'])\n",
    "# y = new_train['target']\n",
    "# SEED = 2022\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c304a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Shape of training data:\", str(X_train.shape))\n",
    "# print(\"Shape of validation data:\", str(X_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267f752",
   "metadata": {},
   "source": [
    "### Preprocess the data into input_ids and attention masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca203726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# def process_data(df, max_length):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "    \n",
    "#     for sentence in df['new_text']:\n",
    "#         encoded_sentence = tokenizer.encode(sentence, add_special_tokens = True)\n",
    "#         input_ids.append(encoded_sentence)\n",
    "        \n",
    "#     input_ids = pad_sequences(input_ids, maxlen = max_length, dtype = 'long', value = 0, \n",
    "#                              truncating = 'post', padding = 'post')\n",
    "    \n",
    "#     for sentence in input_ids:\n",
    "#         attention_masks.append([1 if id > 0 else 0 for id in sentence])\n",
    "                                \n",
    "#     return input_ids, np.array(attention_masks, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcf8a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs, train_attn_masks = process_data(X_train, MAX_LEN)\n",
    "# val_inputs, val_attn_masks = process_data(X_val, MAX_LEN)\n",
    "# test_inputs, test_attn_masks = process_data(test, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "576bac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Train input size: \", str(train_inputs.shape))\n",
    "# print(\"Validation input size: \", str(val_inputs.shape))\n",
    "# print(\"Test input size: \", str(test_inputs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f2d5d",
   "metadata": {},
   "source": [
    "### Convert to numpy int64, then into pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37057c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs = train_inputs.astype(np.int64)\n",
    "# val_inputs = val_inputs.astype(np.int64)\n",
    "# test_inputs = test_inputs.astype(np.int64)\n",
    "\n",
    "# train_labels = np.array(y_train, dtype = np.int64)\n",
    "# val_labels = np.array(y_val, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9416fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs = torch.tensor(train_inputs)\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# train_masks = torch.tensor(train_attn_masks)\n",
    "\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# val_labels = torch.tensor(val_labels)\n",
    "# val_masks = torch.tensor(val_attn_masks)\n",
    "\n",
    "# test_inputs = torch.tensor(test_inputs)\n",
    "# test_masks = torch.tensor(test_attn_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec78965",
   "metadata": {},
   "source": [
    "### Batch the data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e26d5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# batch_size = 16\n",
    "\n",
    "# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "# val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "# val_sampler = RandomSampler(val_data)\n",
    "# val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e842a",
   "metadata": {},
   "source": [
    "### Initializing the model, optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45f47b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     'bert-base-uncased',\n",
    "#     num_labels = 2,\n",
    "#     output_attentions = False,\n",
    "#     output_hidden_states = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99b96bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "# model.to(device)\n",
    "\n",
    "# optimizer = AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr = 2e-5,\n",
    "#     eps = 1e-8\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44192ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# epochs = 10\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps = 0, # default value in run glue.py\n",
    "#     num_training_steps = total_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc71dba",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1a15236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flat_accuracy(preds, labels):\n",
    "#     pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "#     labels_flat = labels.flatten()\n",
    "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e2e2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_time(elapsed):\n",
    "#     elapsed_rounded = int(round((elapsed)))\n",
    "#     return str(datetime.timedelta(seconds = elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78192195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# loss_values = []\n",
    "\n",
    "# for epoch in range(0, epochs):\n",
    "#     print(\"Epoch {:} / {:}\".format(epoch, epochs))\n",
    "#     print(\"Training...\")\n",
    "    \n",
    "#     t0 = time.time()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     model.train()\n",
    "    \n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         if step % 100 == 0 and not step == 0:\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "#             print(\"Batch {:>5,} of {:>5,}. Elapsed: {:}\".format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         batch_input_ids, batch_input_masks, batch_labels = batch\n",
    "\n",
    "#         model.zero_grad()\n",
    "\n",
    "#         outputs = model(batch_input_ids, token_type_ids = None, attention_mask = batch_input_masks,\n",
    "#                        labels = batch_labels)\n",
    "\n",
    "#         loss = outputs[0]\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#     average_train_loss = total_loss / len(train_dataloader)\n",
    "#     loss_values.append(average_train_loss)\n",
    "    \n",
    "#     print(\"\")\n",
    "#     print(\"Average training loss: {0:.5f}\".format(average_train_loss))\n",
    "#     print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "#     print(\"\")\n",
    "#     print(\"Running Validation\")\n",
    "#     t0 = time.time()\n",
    "#     model.eval()\n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "#     for batch in val_dataloader:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         batch_input_ids, batch_input_masks, batch_labels = batch\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(batch_input_ids, token_type_ids = None, \n",
    "#                             attention_mask = batch_input_masks)\n",
    "        \n",
    "#         logits = outputs[0]\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = batch_labels.to('cpu').numpy()\n",
    "        \n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "#         nb_eval_steps += 1\n",
    "    \n",
    "#     print(\"\")\n",
    "#     print(\"Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
    "#     print(\"Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "#     print(\"\")\n",
    "\n",
    "# print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ec0441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(style='darkgrid')\n",
    "# sns.set(font_scale = 1.5)\n",
    "# plt.rcParams['figure.figsize'] = (12,6)\n",
    "# plt.plot(loss_values, 'b-o')\n",
    "# plt.title(\"Training Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec090108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = TensorDataset(test_inputs, test_masks)\n",
    "# test_sampler = SequentialSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)\n",
    "\n",
    "# print(\"Predicting labels for {:,} test sentences...\".format(len(test_inputs)))\n",
    "\n",
    "# model.eval() # SET THE MODEL TO EVALUATION MODE\n",
    "\n",
    "# predictions, true_labels = [], []\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# for (step, batch) in enumerate(test_dataloader):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "#     # Progress update every 100 batches\n",
    "#     if step % 100 == 0 and not step == 0:\n",
    "#         elapse = format_time(time.time() - t0)\n",
    "#         print(\"Batch {:>5,} of {:>5,}. Elapse {:}.\".format(step, len(test_dataloader), elapsed))\n",
    "    \n",
    "#     # unpack the inputs from our dataloader\n",
    "#     b_input_ids, b_input_mask = batch\n",
    "    \n",
    "#     # Telling the model not to compute or store gradients, saving memeory and speeding up prediction\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n",
    "    \n",
    "#     logits = outputs[0]\n",
    "    \n",
    "#     # Move logits and labels to CPU\n",
    "#     logits = logits.detach().cpu().numpy()\n",
    "# #     label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "#     # Store predictions and true labels\n",
    "#     predictions.append(logits)\n",
    "# #     true_labels.append(label_ids)\n",
    "\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fd98cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.concatenate(predictions, axis = 0)\n",
    "# predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "703fe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# batch_size = 32\n",
    "\n",
    "# train_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "# test_data = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster-tweets",
   "language": "python",
   "name": "disaster-tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
